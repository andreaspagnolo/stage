{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c885f2b-9815-471f-8969-5c19c7ab89df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ,  1.        ,\n",
       "        75.12655988, 74.12826721, 73.1300212 , 73.1300212 , 76.12489737],\n",
       "       [ 1.        ,  0.        ,  1.        ,  2.        ,  0.        ,\n",
       "        74.12826721, 73.1300212 , 72.13182377, 72.13182377, 75.12655988],\n",
       "       [ 0.        ,  1.        ,  0.        ,  1.        ,  1.        ,\n",
       "        75.12655988, 74.12826721, 73.1300212 , 73.1300212 , 76.12489737],\n",
       "       [ 1.        ,  2.        ,  1.        ,  0.        ,  2.        ,\n",
       "        76.12489737, 75.12655988, 74.12826721, 74.12826721, 77.12327794],\n",
       "       [ 1.        ,  0.        ,  1.        ,  2.        ,  0.        ,\n",
       "        74.12826721, 73.1300212 , 72.13182377, 72.13182377, 75.12655988],\n",
       "       [75.12655988, 74.12826721, 75.12655988, 76.12489737, 74.12826721,\n",
       "         0.        ,  1.        ,  2.        ,  2.        ,  1.        ],\n",
       "       [74.12826721, 73.1300212 , 74.12826721, 75.12655988, 73.1300212 ,\n",
       "         1.        ,  0.        ,  1.        ,  1.        ,  2.        ],\n",
       "       [73.1300212 , 72.13182377, 73.1300212 , 74.12826721, 72.13182377,\n",
       "         2.        ,  1.        ,  0.        ,  0.        ,  3.        ],\n",
       "       [73.1300212 , 72.13182377, 73.1300212 , 74.12826721, 72.13182377,\n",
       "         2.        ,  1.        ,  0.        ,  0.        ,  3.        ],\n",
       "       [76.12489737, 75.12655988, 76.12489737, 77.12327794, 75.12655988,\n",
       "         1.        ,  2.        ,  3.        ,  3.        ,  0.        ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "06e2ce5d-1733-42e1-a95e-0a4cd33f0ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  education  cancer_stage\n",
      "9   81    1          4             4\n",
      "1    6    0          1             1\n",
      "6   79    1          4             4\n",
      "3    4    0          1             1\n",
      "8   78    1          4             4\n",
      "0    5    0          1             1\n",
      "5   80    1          4             4\n",
      "2    5    0          1             1\n",
      "7   78    1          4             4\n",
      "4    6    0          1             1\n",
      "        age  sex  education  cancer_stage\n",
      "9  1.000000  1.0        1.0           1.0\n",
      "1  0.025974  0.0        0.0           0.0\n",
      "6  0.974026  1.0        1.0           1.0\n",
      "3  0.000000  0.0        0.0           0.0\n",
      "8  0.961039  1.0        1.0           1.0\n",
      "0  0.012987  0.0        0.0           0.0\n",
      "5  0.987013  1.0        1.0           1.0\n",
      "2  0.012987  0.0        0.0           0.0\n",
      "7  0.961039  1.0        1.0           1.0\n",
      "4  0.025974  0.0        0.0           0.0\n",
      "Clusters with threshold 0.1: [0, -1, 0, -1, 0, -1, 0, -1, 0, -1]\n",
      "\n",
      "Silhouette Scores:\n",
      "0.9914807964068715\n",
      "\n",
      "Entropy Scores:\n",
      "0.0\n",
      "\n",
      "Calinski Harabasz Scores:\n",
      "48464.58333333331\n",
      "\n",
      "Davies-Bouldin Scores:\n",
      "90.78698187009488\n",
      "\n",
      "Dunn-Index Scores:\n",
      "75.78093427769282\n",
      "\n",
      "Soglia: 0.1, ARI: 1.0\n",
      "Clusters with threshold 0.33: [0, -1, 0, -1, 0, -1, 0, -1, 0, -1]\n",
      "\n",
      "Silhouette Scores:\n",
      "0.9914807964068715\n",
      "0.9914807964068715\n",
      "\n",
      "Entropy Scores:\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Calinski Harabasz Scores:\n",
      "48464.58333333331\n",
      "48464.58333333331\n",
      "\n",
      "Davies-Bouldin Scores:\n",
      "90.78698187009488\n",
      "90.78698187009488\n",
      "\n",
      "Dunn-Index Scores:\n",
      "75.78093427769282\n",
      "\n",
      "50.52062285180065\n",
      "\n",
      "Soglia: 0.33, ARI: 1.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "#library for Entropy calculation\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#library for Calinski-Harabasz score calculation\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "#library for davies_bouldin score calculation\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "#library for table style\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from gap_statistic import OptimalK\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "#function for dunn_index score calculation\n",
    "def dunn_index(X, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    min_intercluster_distance = np.inf\n",
    "    max_intracluster_diameter = -np.inf\n",
    "\n",
    "    for i in range(len(unique_labels)):\n",
    "        for j in range(i + 1, len(unique_labels)):\n",
    "            cluster_i_points = X[labels == unique_labels[i]]\n",
    "            cluster_j_points = X[labels == unique_labels[j]]\n",
    "            \n",
    "            # Calculate the minimum distance between clusters\n",
    "            intercluster_distance = np.min(pairwise_distances(cluster_i_points, cluster_j_points))\n",
    "            min_intercluster_distance = min(min_intercluster_distance, intercluster_distance)\n",
    "\n",
    "        # Calculate the maximum intra-cluster diameter\n",
    "        intracluster_diameter = np.max(pairwise_distances(X[labels == unique_labels[i]]))\n",
    "        max_intracluster_diameter = max(max_intracluster_diameter, intracluster_diameter)\n",
    "    \n",
    "        return min_intercluster_distance / max_intracluster_diameter\n",
    "\n",
    "#set plot style with default seaborn style\n",
    "sns.set()\n",
    "\n",
    "silhouette_scores = []  \n",
    "entropy_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "davies_bouldin_scores = []\n",
    "dunn_index_scores = [] \n",
    "gap_scores = []\n",
    "\n",
    "tempo_di_calcolo_silhouette = 0\n",
    "tempo_di_calcolo_entropy = 0\n",
    "tempo_di_calcolo_calinski_harabasz = 0\n",
    "tempo_di_calcolo_davies_bouldin = 0\n",
    "tempo_di_calcolo_dunn_index = 0\n",
    "tempo_di_calcolo_gap = 0\n",
    "\n",
    "#variable for metric trend\n",
    "silhouette_appo = 0\n",
    "entropy_appo = 0\n",
    "calinski_harabasz_appo = 0\n",
    "davies_bouldin_appo = 0\n",
    "dunn_index_appo = 0\n",
    "gap_appo = 0\n",
    "\n",
    "data = {\n",
    "    'age': [5, 6, 5, 4, 6, 80, 79, 78, 78, 81],\n",
    "    'sex': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
    "    'education': [1, 1, 1, 1, 1, 4, 4, 4, 4, 4],\n",
    "    'cancer_stage': [1, 1, 1, 1, 1, 4, 4, 4, 4, 4]\n",
    "}\n",
    "\n",
    "#Creation of the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a list with row indices\n",
    "row_indices = list(range(len(df)))\n",
    "\n",
    "# Shuffle index list randomly\n",
    "random.shuffle(row_indices)\n",
    "\n",
    "# Reorder the DataFrame with the new row order\n",
    "df = df.iloc[new_order]\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Normalize each column\n",
    "df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "print(df)\n",
    "\n",
    "#thresholds for clustering\n",
    "soglie = [0.1, 0.33]\n",
    "\n",
    "for soglia in soglie:\n",
    "    # Calcolo delle distanze euclidee\n",
    "    euclidean_distances= pdist(df.values, metric='euclidean')\n",
    "    distance_matrix = squareform(euclidean_distances)\n",
    "    \n",
    "    cluster_labels_euclidean = []\n",
    "    \n",
    "    # Initialize an array to store cluster labels\n",
    "    cluster_labels = [-1] * len(df)\n",
    "    current_label = 0\n",
    "    \n",
    "    # Assign cluster labels based on the threshold\n",
    "    #parte da sistemare\n",
    "    for i in range(len(df) and current_label < 2):\n",
    "        if cluster_labels[i] == -1:\n",
    "            cluster_labels[i] = current_label\n",
    "            for j in range(i+1, len(df)):\n",
    "                if distance_matrix[i, j] < soglia and cluster_labels[j] == -1:\n",
    "                    cluster_labels[j] = current_label\n",
    "            current_label += 1\n",
    "    \n",
    "    # Print cluster labels\n",
    "    print(f\"Clusters with threshold {soglia}: {cluster_labels}\")\n",
    "    \n",
    "    # Calcolo del clustering con k-means\n",
    "    kmeans = KMeans(n_clusters=2)\n",
    "    kmeans_clusters = kmeans.fit_predict(df)\n",
    "    l= kmeans.labels_\n",
    "\n",
    "    # Inizializza una lista per tenere traccia dei tempi di calcolo\n",
    "    tempi_di_calcolo = np.empty((6, 7))\n",
    "\n",
    "    # Silhouette calculation\n",
    "    start_time = time.time()\n",
    "    silhouette_avg = silhouette_score(df, l)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    end_time = time.time()\n",
    "    tempo_di_calcolo = end_time - start_time\n",
    "    tempo_di_calcolo_silhouette = tempo_di_calcolo_silhouette + tempo_di_calcolo\n",
    "    \n",
    "    # Entropy calculation (complementary Entropy)\n",
    "    start_time = time.time()\n",
    "    entropy_val = entropy(np.bincount(l) / len(l), base=2)\n",
    "    entropy_val = abs(1-entropy_val)\n",
    "    entropy_scores.append(entropy_val)\n",
    "    end_time = time.time()\n",
    "    tempo_di_calcolo = end_time - start_time\n",
    "    tempo_di_calcolo_entropy = tempo_di_calcolo_entropy + tempo_di_calcolo\n",
    "\n",
    "    # Calinski-Harabasz index calculation\n",
    "    start_time = time.time()\n",
    "    calinski_harabasz_index = calinski_harabasz_score(df, l)\n",
    "    calinski_harabasz_scores.append(calinski_harabasz_index)\n",
    "    end_time = time.time()\n",
    "    tempo_di_calcolo = end_time - start_time\n",
    "    tempo_di_calcolo_calinski_harabasz = tempo_di_calcolo_calinski_harabasz + tempo_di_calcolo\n",
    "\n",
    "    # Davies-Bouldin calculation (reciprocal function)\n",
    "    start_time = time.time()\n",
    "    davies_bouldin_avg = davies_bouldin_score(df, l)\n",
    "    davies_bouldin_avg = 1 / davies_bouldin_avg\n",
    "    davies_bouldin_scores.append(davies_bouldin_avg)\n",
    "    end_time = time.time()\n",
    "    tempo_di_calcolo = end_time - start_time\n",
    "    tempo_di_calcolo_davies_bouldin = tempo_di_calcolo_davies_bouldin + tempo_di_calcolo\n",
    "\n",
    "    # Dunn index calculation\n",
    "    start_time = time.time()\n",
    "    dunn_score = dunn_index(df, l)\n",
    "    dunn_index_scores.append(dunn_score)\n",
    "    end_time = time.time()\n",
    "    tempo_di_calcolo = end_time - start_time\n",
    "    tempo_di_calcolo_dunn_index = tempo_di_calcolo_dunn_index + tempo_di_calcolo\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"Silhouette Scores:\")\n",
    "    for score in silhouette_scores:\n",
    "        print(score)\n",
    "    \n",
    "    print(\"\\nEntropy Scores:\")\n",
    "    for score in entropy_scores:\n",
    "        print(score)\n",
    "        \n",
    "    print(\"\\nCalinski Harabasz Scores:\")\n",
    "    for score in calinski_harabasz_scores:\n",
    "        print(score)\n",
    "    \n",
    "    print(\"\\nDavies-Bouldin Scores:\")\n",
    "    for score in davies_bouldin_scores:\n",
    "        print(score)\n",
    "        \n",
    "    print(\"\\nDunn-Index Scores:\")\n",
    "    for score in dunn_index_scores:\n",
    "        print(score)\n",
    "        print(\"\")\n",
    "\n",
    "    # Applicazione dell'Adjusted Rand Index\n",
    "    ari = adjusted_rand_score(cluster_labels, kmeans_clusters)\n",
    "    print(f\"Soglia: {soglia}, ARI: {ari}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
